{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Explainble Models with LIME\n",
    "\n",
    "_Authors: Greg Baker (SYD) and Justin Pounders_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Define some of the political, ethical and economic ramifications of using black-box models in data science.\n",
    "- Describe the features of explainable vs non-explainable (black-box) models\n",
    "- Define and discuss the global implications of the EU GDPR\n",
    "- Use LIME to explain a black-box model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Introduction: legal and ethical responsibilities](#intro)\n",
    "- [Black-box models and explainability](#blackbox)\n",
    "- [GDPR](#gdpr)\n",
    "- [Options for Machine Learning on EU Citizen Data](#options)\n",
    "- [LIME Example](#lime)\n",
    "- [Resources](#resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction: legal and ethical responsibilities\n",
    "\n",
    "As data scientists, we often have access to very personal information about \n",
    "our customers or users. \n",
    "\n",
    "Many countries have very strict laws on how such data must be handled.\n",
    "\n",
    "### Warning\n",
    "\n",
    "Laws change -- sometimes quite suddenly -- and General Assembly doesn't\n",
    "promise that anything in this topic won't be out-of-date by the time you\n",
    "finish the class.\n",
    "\n",
    "Also, _I AM NOT A LAWYER_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we care?\n",
    "\n",
    "![](assets/chowdhury.png)\n",
    "\n",
    "> Taken from Rumman Chowdhury's presentation at the Southern Data Science Conference, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OKCupid\n",
    "\n",
    "![](assets/okcupid.png)\n",
    "\n",
    "> http://fortune.com/2014/07/28/okcupid-we-experiment-on-users-too/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambridge Analytica\n",
    "\n",
    "![](assets/cambridge1.png)\n",
    "\n",
    "---\n",
    "\n",
    "![](assets/cambridge2.png)\n",
    "\n",
    "> https://www.nytimes.com/2018/03/19/technology/facebook-cambridge-analytica-explained.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "\n",
    "## Racial \"profiling\" by country\n",
    "\n",
    "<img src=\"assets/France.png\" style=\"float: left; margin: 10px; height: 50px\">\n",
    "\n",
    "### France - you must not ask about race\n",
    "\n",
    "<br>\n",
    "\n",
    "<blockquote>Race is such a taboo term that a 1978 law specifically banned the collection and computerized storage of race-based data without the express consent of the interviewees or a waiver by a state committee. France therefore collects no census or other data on the race (or ethnicity) of its citizens.  [Race policy in France](https://www.brookings.edu/articles/race-policy-in-france/)\n",
    "</blockquote>\n",
    "\n",
    "Even if you don't use race in your model, you simply can't store this in a database or even\n",
    "have it appear in a pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "----\n",
    "\n",
    "<img src=\"assets/malaysia.png\" style=\"float: left; margin: 10px; height: 50px\">\n",
    "\n",
    "### Malaysia - you must ask about race\n",
    "\n",
    "<br>\n",
    "\n",
    "<blockquote>\n",
    "Race is such an important term in Malaysia and the history of discrimination so\n",
    "strong, that there are great efforts to make sure that companies aren't being discriminatory \n",
    "or racist. It is quite\n",
    "common to collect some quite personal data about every customer, student or user:\n",
    "\n",
    "<ul>\n",
    "<li> _What is your race?_\n",
    "<li> _What is your religion?_\n",
    "</ul>\n",
    "\n",
    "These are written on everyone's national identity card and stored in numerous databases.\n",
    "Changing religion requires completing a formal application and being issued with a new \n",
    "identity card.\n",
    "</blockquote>\n",
    "\n",
    "If you are dealing with data from Malaysia it will be important to show that any algorithm you \n",
    "use doesn't adversely affect people of different religions as a court can easily request data \n",
    "on your per-race outcomes. Analysing student education \n",
    "outcomes to confirm no racial or religius bias is important, for example.\n",
    "\n",
    "So you will definitely need to do generate some descriptive statistics on race, for example.\n",
    "\n",
    "### Side-note:\n",
    "\n",
    "This makes it very difficult for a French university to open a campus in Malaysia!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the underlying ethical motivation, and how does it affect data scientists?\n",
    "\n",
    "\n",
    "- Generally, countries set up laws so that the colour of your skin or what you believe shouldn't affect how\n",
    "you are treated by other human beings.\n",
    "- But many decisions are now being made by _algorithms_, rather than human beings.\n",
    "\n",
    "> That's the first rule of algorithms.\n",
    "> Algorithms are **opinions embedded in code**.\n",
    "> ...\n",
    "> (As data scientists), we should not be the arbiters of truth. We should be translators of ethical discussions that happen in larger society.\n",
    "> --- [Cathy O'Neill TED Talk](https://www.ted.com/talks/cathy_o_neil_the_era_of_blind_faith_in_big_data_must_end/transcript)\n",
    "\n",
    "- Data scientists have a unique responsibility to make algorithms that **treat everyone fairly**, just as you would\n",
    "expect a human being to treat everyone fairly.\n",
    "- In Europe, this is now a **legal responsibility**, which will mostly fall on data scientists: justify why your\n",
    "model is reasonable and fair.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Considerations\n",
    "\n",
    "- Privacy/anonymity\n",
    "- Data ownership\n",
    "- Consent\n",
    "- Explainability\n",
    "\n",
    "> We are going to focus on explainability for the rest of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"blackbox\"></a>\n",
    "\n",
    "----\n",
    "\n",
    "## What is a black-box model?\n",
    "\n",
    "A **black-box model** is a machine learning model where it is hard to explain to a non-technical audience how or why it works. It's like looking into a very dark box where you can't see anything.\n",
    "\n",
    "**An explainable model** is the opposite of a black-box model:\n",
    "\n",
    "- The workings of the algorithm aren't secret intellectual property\n",
    "\n",
    "- It is possible for a human being to take the result of training and calculate predictions by hand.\n",
    "\n",
    "- It only uses a small number of attributes to make its decision -- too many and it's hard for a human being\n",
    "  to think through their interactions.\n",
    "\n",
    "- The attributes the algorithm uses to make a prediction seem relevant to the problem at hand\n",
    "\n",
    "\n",
    "Some explainable examples:\n",
    "\n",
    "- A linear model $y = a_0 x_0 + a_1 x_y + b$ can be computed by hand even if it required a computer\n",
    "  to identify the optimal $a_0$ and $a_1$\n",
    "  \n",
    "- A decision tree with a depth of 3 (8 nodes) can be traced by hand even if it required a computer\n",
    "  to find the best variables to split on\n",
    "  \n",
    "Some black-box models:\n",
    "  \n",
    "- A random forest used the average answer from a thousand different decision trees would be hard to calculate by hand\n",
    "\n",
    "- The k-nearest-neighbours algorithm is conceptually simple but would be hard to calculate by hand. Also it uses all the attributes of each data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Knowledge check\n",
    "\n",
    "1. When should we _not_ use black-box models?\n",
    "\n",
    "2. Based on your knowledge of these algorithms, tag them as being \"easy to explain\" or \"a bit of a black-box\"\n",
    "\n",
    "  - Logistic regression\n",
    "\n",
    "  - Deep learning\n",
    "\n",
    "  - Random forests\n",
    "\n",
    "  - Bayesian methods\n",
    "\n",
    "  - Decision trees\n",
    "\n",
    "  - Rule lists (sequences of if-then statements)\n",
    "  \n",
    "  - k-Nearest-Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='gdpr'></a>\n",
    "\n",
    "----\n",
    "\n",
    "<img src=\"assets/eu.png\" style=\"float: left; margin: 20px; height: 50px\">\n",
    "\n",
    "# EU GDPR\n",
    "\n",
    "<br>\n",
    "\n",
    "The General Data Protection Regulation was agreed to in April 2016. It went into effect on **25th May 2018**.\n",
    "\n",
    "It puts some very serious constraints on what can and can't be kept in a database, and makes some\n",
    "common machine learning algorithms difficult to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Article 22\n",
    "\n",
    "<blockquote>\n",
    "<p>\n",
    "The data subject shall have the right not to be subject to a **decision based solely on automated processing**, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "The data controller shall implement suitable measures to safeguard the data subject's rights and freedoms and legitimate interests, at least the **right to obtain human intervention** on the part of the controller, to express his or her point of view and to contest the decision. \n",
    "</p>\n",
    "</blockquote>\n",
    "\n",
    "Since a machine learning classifier is definitely a \"decision based solely on automated processing\", this means that EU citizens have the right to ask to bypass that wonderful new machine learning algorithm you created and have a human being make the decision instead.\n",
    "\n",
    "The legislation doesn't strictly define what a \"legal effect\" is, but at the extremes:\n",
    "\n",
    "- Choosing the optimal image for a landing page to match someone's preferences is probably OK and probably not subject to Article 22.\n",
    "\n",
    "- If you have different pricing models for different customers and you use a machine learning algorithm to\n",
    " choose the offer that you think will best suit them (or makes your company the most money!) you are probably subject to\n",
    " Article 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Article 9\n",
    "\n",
    "<blockquote>\n",
    "<p>\n",
    " Processing of personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade union membership ... data concerning health or data concerning a natural person's sex life or sexual orientation shall be prohibited. \n",
    " </p>\n",
    "</blockquote>\n",
    "\n",
    "This applies to _any company, anywhere in the world_ working with data from EU citizens. Your input dataframe ($X$) in \n",
    "your model _can't_ have columns for:\n",
    " \n",
    " - sexual orientation\n",
    " - race\n",
    " - politics\n",
    " - religion\n",
    " \n",
    "You can't even have columns that are strongly correlated with any of these things.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Article 83\n",
    "\n",
    "<blockquote>\n",
    "Infringements of the ... provisions shall ... be subject to administrative fines ... up to 4 % of the total worldwide annual turnover of the preceding financial year\n",
    "</blockquote>\n",
    "\n",
    "For reference, this is the maximum potential fine for various companies:\n",
    "\n",
    "| Company | Annual Revenue | Maximum Fine ||\n",
    "|----------|---------------|---------------|\n",
    "| Microsoft (including LinkedIn) | USD100B | USD 4,000,000,000 |\n",
    "| Alphabet (Google) | USD90B | USD 3,600,000,000 |\n",
    "| Facebook | USD27B | USD 1,080,000,000 |\n",
    "| Netflix | USD8.8B | USD 320,000,000 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recital 71\n",
    "\n",
    "<blockquote>\n",
    "...processing should be subject to suitable safeguards, which should include ... **an explanation of the decision** reached after such assessment and [the option] to challenge the decision.\n",
    "</blockquote>\n",
    "\n",
    "Recitals are not law, they are additional information written out to explain the purposes and thinking of the Articles. It's possible that the EU courts may not interpret the law the same way as the lawmakers did!\n",
    "\n",
    "But this key phrase: _an explanation of the decision_ has caused a lot of angst in the machine learning community.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recital 71 (continued)\n",
    "\n",
    "<blockquote>\n",
    "... the controller should use appropriate mathematical or statistical procedures ... [to take] account of the potential risks involved for the interests and rights of the data subject and that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.\n",
    "</blockquote>\n",
    "\n",
    "Not only do you have to show that you didn't deliberately include race, religion or sexual orientation in your model, *you also have to show that your model isn't discriminating against anyone accidentally*. \n",
    "\n",
    "Of course, this is a bit challenging\n",
    "for your French users, because you won't be able to store what race they identify with in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review questions\n",
    "\n",
    "1. Can you think of any data that you would you not want to store about a person? (Suggestions: think about\n",
    "things that might be illegal or immoral or embarrassing for them if it were in a database that you controlled.)\n",
    "\n",
    "2. If you had very complete information about someone (where they live, every address they have\n",
    "visited in the last year, what sort of car they drive, what they have bought in the last year, every phone number\n",
    "they have called, everyone they have emailed or messaged), how well could\n",
    "you guess their religion, politics, sexual orientation or ancestry? How would you do this?\n",
    "\n",
    "3. If a company or government department made a decision about you that you didn't think was fair, how would you \n",
    "like to have your complaint handled?\n",
    "\n",
    "4. If your company is based outside of the EU, you don't have to worry about the EU GDPR. True or false? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"options\"></a>\n",
    "\n",
    "----\n",
    "\n",
    "# Options for Machine Learning on EU Citizen's Data\n",
    "\n",
    "\n",
    "There are two approaches that are likely to satisfy the GDPR:\n",
    "\n",
    "- Only use machine learning algorithms that can be understood very easily by non-technical users\n",
    "\n",
    "- Retro-fit explainability over other algorithms, including black-box algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name='lime'></a>\n",
    "\n",
    "# LIME\n",
    "\n",
    "You might remember a maths teacher telling you that if you zoom in very close to a typical continuous \n",
    "function it will look linear.\n",
    "\n",
    "No matter how complex a black box model you have created, it is nearly impossible to make something\n",
    "that isn't generally continuous and still be useful.\n",
    "\n",
    "So while we can't always explain how a black box model works in general, if you zoom in very close,\n",
    "you will be able to explain what is happening *locally*.\n",
    "\n",
    "This is known as LIME: **Locally Interpretable Model-Agnostic Explanations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you haven't already install the `lime` package: `pip install lime`.  \n",
    "> The package source code (and some good resources like a video introduction and the original paper) are on [GitHub](https://github.com/marcotcr/lime)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How LIME works\n",
    "\n",
    "Let's say that we have a hard-to-explain model named $H$, and an observation called $x$ that we would like to explain.\n",
    "\n",
    "1. Generate a bunch of \"close-by\" data points by randomly altering the values of features.  Let's call these \"perturbed data.\"\n",
    "2. For each perturbed data point, predict the label using your hard-to-explain model, $H$.\n",
    "3. Gather the perturbed data and their labels from steps 1 and 2.  This is a _new training set_ for an explainable model.\n",
    "4. Train your easy-to-explain model (call it $E$) on your new dataset from 3.\n",
    "5. Use your easy-to-explain model $E$ to predict the label of $x$.\n",
    "6. Because $E$ is easy to explain, you can explain the prediction made for $x$.\n",
    "\n",
    "\n",
    "In the following picture, the bright red cross is the instance being explained, and the other crosses\n",
    "and dots are other nearby data points. Based on this, it will make a linear rule (a straight line that\n",
    "divides the two different background colours). It's obviously not right everywhere, but it is correct\n",
    "close to the big red cross.\n",
    "\n",
    "<img src=\"assets/lime.png\">\n",
    "\n",
    "Our explanation is:\n",
    "\n",
    "- if the red cross had been a little further to the right and down, it would have been classified differently\n",
    "\n",
    "- if the red cross had been a little further to the left and up, it would have been classified the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Who survived the Titanic?\n",
    "\n",
    "If you wanted to understand why your Titanic model predicted that Henry Tornquist\n",
    "(3rd class, male, age 25) should have died (he actually survived), LIME would get a prediction from\n",
    "your model for:\n",
    "\n",
    "- A 3rd class, male, age 24\n",
    "- A 3rd class male, age 26\n",
    "- A 2nd class, male, age 25\n",
    "- A 3rd class, female, age 25\n",
    "\n",
    "And so on -- a variety of similar, but not-quite-the-same people. LIME would then discover that\n",
    "\n",
    "- being male instead of female made a big difference\n",
    "- being third class instead of 2nd class made a big difference\n",
    "- being a little older or younger made very little difference\n",
    "\n",
    "It would explain your model as \"mostly responding to Sex and Pclass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.neighbors\n",
    "import sklearn.model_selection\n",
    "\n",
    "knn = sklearn.neighbors.KNeighborsClassifier()\n",
    "titanic = pd.read_csv('datasets/titanic.csv', index_col=0)\n",
    "titanic = pd.DataFrame({\n",
    "    'Age': titanic.Age / titanic.Age.mean(),\n",
    "    'Pclass': titanic.Pclass,\n",
    "    'Sex': (titanic.Sex == 'male').astype(int),\n",
    "    'Survived': titanic.Survived\n",
    "}).dropna().values\n",
    "\n",
    "#titanic\n",
    "\n",
    "(Xtrain, Xtest, Ytrain, Ytest) = sklearn.model_selection.train_test_split(titanic[:,:3], titanic[:,3], random_state=42)\n",
    "knn.fit(Xtrain, Ytrain)\n",
    "knn.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a baseline\n",
    "values, counts = np.unique(Ytrain, return_counts=True) \n",
    "counts/len(Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(Xtrain,\n",
    "                                                   feature_names = ['Age', 'Pclass', 'Sex'],\n",
    "                                                   class_names = ['Died', 'Survived'],\n",
    "                                                   discretize_continuous=True)\n",
    "explanation = explainer.explain_instance(np.array([25, 3, 1]),\n",
    "                           knn.predict_proba,\n",
    "                           num_features=2\n",
    "                          )\n",
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most significant features are:\n",
    " - Mr Tornquist's Sex (greater than 0 means male)\n",
    " - His ticket class (greater than 2 means third class)\n",
    " \n",
    "We could write this as:\n",
    "\n",
    "  $survival = -0.47 * maleness - 0.26 * thirdclassness$\n",
    "  \n",
    "For passengers who are of a similar age to Mr Tornquist, this provides a reasonable quick rule\n",
    "of thumb about whether they would be more likely to survive or less like to survive than Mr Tornquist.\n",
    "Be less male or buy a different ticket if you want to survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who's on first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['rec.sport.baseball', \n",
    "              'rec.sport.hockey']\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's baseline?\n",
    "values, counts = np.unique(data_train.target, return_counts=True)\n",
    "counts/len(data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "components = [('vectorizer', TfidfVectorizer(stop_words='english')),\n",
    "              ('clf', SVC(gamma=1., C=1.0, probability=True))]\n",
    "\n",
    "pipe = Pipeline(components)\n",
    "pipe.fit(data_train.data, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(data_train.data, data_train.target), pipe.score(data_test.data, data_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "idx = 2\n",
    "\n",
    "exp = explainer.explain_instance(data_test.data[idx], pipe.predict_proba, num_features=6)\n",
    "print('Document id: %d' % idx)\n",
    "print('Probability(hockey) =', pipe.predict_proba([data_test.data[idx]])[0,1])\n",
    "print('True class: %s' % categories[data_test.target[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- [Is there are a right to explanation for Machine Learning in the GDPR?](https://iapp.org/news/a/is-there-a-right-to-explanation-for-machine-learning-in-the-gdpr)\n",
    "- [Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2903469)\n",
    "- [Towards Interpretable Reliable Models](https://blog.kjamistan.com/towards-interpretable-reliable-models/)\n",
    "- [GDPR and you](https://blog.kjamistan.com/gdpr-you-my-talk-at-cloudera-sessions-munchen/)\n",
    "- [Hold Your Machine Learning and AI Models Accountable](https://medium.com/pachyderm-data/hold-your-machine-learning-and-ai-models-accountable-de887177174c)\n",
    "- [How GDPR Affects Data Science](https://kdnuggets.com/2017/07/gdpr-affects-data-science.html)\n",
    "- [Scaleable Bayesian rule lists](https://arxiv.org/pdf/1602.08610v2.pdf)\n",
    "\n",
    "- [Why Should I Trust You? Explaining the Predictions of Any Classfier ](https://www.youtube.com/watch?v=hUnRCxnydCc)\n",
    "- [Explaining Complex Machine Learning Models with LIME](https://datascienceplus.com/explaining-complex-machine-learning-models-with-lime/)\n",
    "\n",
    "---\n",
    "\n",
    "Other methods that can be used to support model explainability:\n",
    "\n",
    "- [Shapley values](https://en.wikipedia.org/wiki/Shapley_value) emerged from game theory.  They have recently been applied to machine learning to explain which features were important in arriving at a prediction.  There is also a Python implementation: [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap).\n",
    "- [Path Specific Counterfactual Fairness](https://arxiv.org/abs/1802.08139) is a recent paper that looks at _direct_ and _indirect_ effects to determine whether certain features are actually influencing a prediction.  For example, this method can be used to demonstrate that gender bias was apparently _not_ a factor in Berkley admissions because the \"counterfactual\" case (in which the gender features were switched) did not affect the prediction.\n",
    "- [Generating Visual Explanations](https://arxiv.org/abs/1603.08507) uses a combination of images and text to automatically generate _useful_ textual descriptions of why predictions were made in image classification.\n",
    "\n",
    "![](assets/viz_exp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
